{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and cleaning the data\n",
    "#Dataset- DUC2001\n",
    "#Importing the libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_DUC_document(path: str):\n",
    "    '''\n",
    "    Read a DUC document from a given path.\n",
    "    '''\n",
    "    exception_count = 0\n",
    "    files = os.listdir(os.path.join(path, 'documents'))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    DUC = pd.DataFrame(columns=['Document', 'Summary', 'Text'])\n",
    "    for i in range(len(files)):\n",
    "        print('cleaning document: ', files[i])\n",
    "        with open(os.path.join(path,'documents',files[i]),'r') as f:\n",
    "            file = f.read()\n",
    "            # file = file.replace('\\n',' ')\n",
    "            file = file.replace('\\'','')\n",
    "            file = file.replace('\\\"','')\n",
    "            file = file.replace('`','')\n",
    "            file = RegexpTokenizer('<TEXT>(.*?)</TEXT>').tokenize(file)\n",
    "            file = \" \".join(file)\n",
    "            # file = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", file)\n",
    "            file = file.lower()\n",
    "            file = file.split()\n",
    "            file = [w for w in file if not w in stop_words]\n",
    "            file = [ps.stem(w) for w in file]\n",
    "            file = [lemmatizer.lemmatize(w) for w in file]\n",
    "            file = [w for w in file if not w in stop_words]\n",
    "            file = \" \".join(file)\n",
    "            try:\n",
    "                with open(os.path.join(path,'Summaries',files[i].lower()+'.txt'),'r') as summary_reader:\n",
    "                    summary = summary_reader.readline()\n",
    "                    summary = summary_reader.readline()\n",
    "                    summary = summary.replace('\\n','')\n",
    "                    summary = summary.replace('Abstract:','')\n",
    "            except Exception as e:\n",
    "                exception_count += 1\n",
    "                summary = ''\n",
    "            DUC.loc[i] = [files[i], summary, file]\n",
    "    print('Number of exceptions: ', exception_count)\n",
    "    return DUC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning document:  AP830325-0143\n",
      "cleaning document:  AP880217-0175\n",
      "cleaning document:  AP880318-0051\n",
      "cleaning document:  AP880330-0119\n",
      "cleaning document:  AP880331-0140\n",
      "cleaning document:  AP880409-0015\n",
      "cleaning document:  AP880419-0131\n",
      "cleaning document:  AP880510-0178\n",
      "cleaning document:  AP880517-0226\n",
      "cleaning document:  AP880520-0264\n",
      "cleaning document:  AP880601-0040\n",
      "cleaning document:  AP880613-0161\n",
      "cleaning document:  AP880623-0135\n",
      "cleaning document:  AP880629-0159\n",
      "cleaning document:  AP880630-0295\n",
      "cleaning document:  AP880705-0006\n",
      "cleaning document:  AP880705-0018\n",
      "cleaning document:  AP880705-0109\n",
      "cleaning document:  AP880714-0142\n",
      "cleaning document:  AP880801-0195\n",
      "cleaning document:  AP880811-0299\n",
      "cleaning document:  AP880816-0234\n",
      "cleaning document:  AP880901-0052\n",
      "cleaning document:  AP880902-0062\n",
      "cleaning document:  AP880903-0092\n",
      "cleaning document:  AP880913-0129\n",
      "cleaning document:  AP880913-0204\n",
      "cleaning document:  AP880914-0027\n",
      "cleaning document:  AP880914-0079\n",
      "cleaning document:  AP880926-0203\n",
      "cleaning document:  AP880927-0089\n",
      "cleaning document:  AP880927-0117\n",
      "cleaning document:  AP880928-0054\n",
      "cleaning document:  AP880928-0146\n",
      "cleaning document:  AP881009-0072\n",
      "cleaning document:  AP881017-0235\n",
      "cleaning document:  AP881018-0136\n",
      "cleaning document:  AP881126-0007\n",
      "cleaning document:  AP881206-0114\n",
      "cleaning document:  AP881210-0115\n",
      "cleaning document:  AP881211-0027\n",
      "cleaning document:  AP881216-0017\n",
      "cleaning document:  AP881222-0089\n",
      "cleaning document:  AP881222-0119\n",
      "cleaning document:  AP881222-0126\n",
      "cleaning document:  AP881227-0185\n",
      "cleaning document:  AP890111-0217\n",
      "cleaning document:  AP890111-0227\n",
      "cleaning document:  AP890117-0132\n",
      "cleaning document:  AP890131-0280\n",
      "cleaning document:  AP890227-0016\n",
      "cleaning document:  AP890228-0019\n",
      "cleaning document:  AP890302-0063\n",
      "cleaning document:  AP890307-0150\n",
      "cleaning document:  AP890313-0198\n",
      "cleaning document:  AP890314-0237\n",
      "cleaning document:  AP890316-0018\n",
      "cleaning document:  AP890322-0010\n",
      "cleaning document:  AP890325-0029\n",
      "cleaning document:  AP890326-0081\n",
      "cleaning document:  AP890403-0123\n",
      "cleaning document:  AP890404-0260\n",
      "cleaning document:  AP890501-0176\n",
      "cleaning document:  AP890502-0205\n",
      "cleaning document:  AP890511-0126\n",
      "cleaning document:  AP890529-0030\n",
      "cleaning document:  AP890704-0043\n",
      "cleaning document:  AP890708-0135\n",
      "cleaning document:  AP890714-0129\n",
      "cleaning document:  AP890719-0225\n",
      "cleaning document:  AP890722-0081\n",
      "cleaning document:  AP890801-0025\n",
      "cleaning document:  AP890802-0064\n",
      "cleaning document:  AP890803-0008\n",
      "cleaning document:  AP890805-0126\n",
      "cleaning document:  AP890907-0221\n",
      "cleaning document:  AP890922-0167\n",
      "cleaning document:  AP890930-0100\n",
      "cleaning document:  AP891006-0029\n",
      "cleaning document:  AP891017-0204\n",
      "cleaning document:  AP891028-0022\n",
      "cleaning document:  AP891116-0115\n",
      "cleaning document:  AP891116-0191\n",
      "cleaning document:  AP891201-0100\n",
      "cleaning document:  AP891210-0079\n",
      "cleaning document:  AP891213-0004\n",
      "cleaning document:  AP900215-0031\n",
      "cleaning document:  AP900217-0078\n",
      "cleaning document:  AP900306-0105\n",
      "cleaning document:  AP900313-0191\n",
      "cleaning document:  AP900316-0028\n",
      "cleaning document:  AP900322-0192\n",
      "cleaning document:  AP900322-0200\n",
      "cleaning document:  AP900323-0036\n",
      "cleaning document:  AP900416-0188\n",
      "cleaning document:  AP900419-0121\n",
      "cleaning document:  AP900424-0035\n",
      "cleaning document:  AP900426-0054\n",
      "cleaning document:  AP900428-0005\n",
      "cleaning document:  AP900428-0108\n",
      "cleaning document:  AP900511-0159\n",
      "cleaning document:  AP900512-0038\n",
      "cleaning document:  AP900521-0063\n",
      "cleaning document:  AP900529-0005\n",
      "cleaning document:  AP900601-0040\n",
      "cleaning document:  AP900607-0039\n",
      "cleaning document:  AP900619-0006\n",
      "cleaning document:  AP900625-0160\n",
      "cleaning document:  AP900629-0260\n",
      "cleaning document:  AP900703-0040\n",
      "cleaning document:  AP900721-0110\n",
      "cleaning document:  AP900829-0120\n",
      "cleaning document:  AP900910-0020\n",
      "cleaning document:  AP901010-0036\n",
      "cleaning document:  AP901012-0032\n",
      "cleaning document:  AP901013-0046\n",
      "cleaning document:  AP901029-0035\n",
      "cleaning document:  AP901030-0216\n",
      "cleaning document:  AP901031-0024\n",
      "cleaning document:  AP901130-0060\n",
      "cleaning document:  AP901203-0166\n",
      "cleaning document:  AP901231-0012\n",
      "cleaning document:  FBIS-41815\n",
      "cleaning document:  FBIS-45908\n",
      "cleaning document:  FBIS3-11919\n",
      "cleaning document:  FBIS3-22942\n",
      "cleaning document:  FBIS3-23360\n",
      "cleaning document:  FBIS3-30788\n",
      "cleaning document:  FBIS3-41\n",
      "cleaning document:  FBIS3-51875\n",
      "cleaning document:  FBIS4-27602\n",
      "cleaning document:  FBIS4-4674\n",
      "cleaning document:  FBIS4-56863\n",
      "cleaning document:  FBIS4-67721\n",
      "cleaning document:  FT911-2650\n",
      "cleaning document:  FT911-3463\n",
      "cleaning document:  FT911-5176\n",
      "cleaning document:  FT921-305\n",
      "cleaning document:  FT921-9310\n",
      "cleaning document:  FT922-10200\n",
      "cleaning document:  FT922-3171\n",
      "cleaning document:  FT922-6646\n",
      "cleaning document:  FT922-8860\n",
      "cleaning document:  FT923-5089\n",
      "cleaning document:  FT923-5267\n",
      "cleaning document:  FT923-5797\n",
      "cleaning document:  FT923-5835\n",
      "cleaning document:  FT923-5859\n",
      "cleaning document:  FT923-6038\n",
      "cleaning document:  FT923-6110\n",
      "cleaning document:  FT923-6455\n",
      "cleaning document:  FT923-7126\n",
      "cleaning document:  FT931-11394\n",
      "cleaning document:  FT931-341\n",
      "cleaning document:  FT931-3883\n",
      "cleaning document:  FT932-12322\n",
      "cleaning document:  FT932-5855\n",
      "cleaning document:  FT933-10881\n",
      "cleaning document:  FT933-2760\n",
      "cleaning document:  FT933-5709\n",
      "cleaning document:  FT933-6011\n",
      "cleaning document:  FT933-8272\n",
      "cleaning document:  FT933-8941\n",
      "cleaning document:  FT934-10911\n",
      "cleaning document:  FT934-11014\n",
      "cleaning document:  FT934-12800\n",
      "cleaning document:  FT934-13350\n",
      "cleaning document:  FT934-5781\n",
      "cleaning document:  FT934-8628\n",
      "cleaning document:  FT934-8748\n",
      "cleaning document:  FT934-9116\n",
      "cleaning document:  FT941-1547\n",
      "cleaning document:  FT941-1750\n",
      "cleaning document:  FT941-4219\n",
      "cleaning document:  FT941-575\n",
      "cleaning document:  FT942-11114\n",
      "cleaning document:  FT943-12341\n",
      "cleaning document:  FT943-4951\n",
      "cleaning document:  FT943-5628\n",
      "cleaning document:  FT944-18184\n",
      "cleaning document:  LA010890-0031\n",
      "cleaning document:  LA011889-0067\n",
      "cleaning document:  LA012090-0090\n",
      "cleaning document:  LA012590-0174\n",
      "cleaning document:  LA021090-0005\n",
      "cleaning document:  LA021689-0227\n",
      "cleaning document:  LA030489-0068\n",
      "cleaning document:  LA030789-0047\n",
      "cleaning document:  LA030889-0163\n",
      "cleaning document:  LA032589-0044\n",
      "cleaning document:  LA032789-0038\n",
      "cleaning document:  LA040689-0056\n",
      "cleaning document:  LA040789-0051\n",
      "cleaning document:  LA041889-0039\n",
      "cleaning document:  LA042190-0060\n",
      "cleaning document:  LA042290-0104\n",
      "cleaning document:  LA042490-0142\n",
      "cleaning document:  LA042790-0205\n",
      "cleaning document:  LA043089-0197\n",
      "cleaning document:  LA050889-0075\n",
      "cleaning document:  LA051190-0185\n",
      "cleaning document:  LA051590-0065\n",
      "cleaning document:  LA052289-0050\n",
      "cleaning document:  LA060490-0083\n",
      "cleaning document:  LA061589-0143\n",
      "cleaning document:  LA070189-0080\n",
      "cleaning document:  LA070190-0073\n",
      "cleaning document:  LA071589-0076\n",
      "cleaning document:  LA071590-0068\n",
      "cleaning document:  LA072089-0140\n",
      "cleaning document:  LA073089-0118\n",
      "cleaning document:  LA080189-0042\n",
      "cleaning document:  LA080790-0111\n",
      "cleaning document:  LA081489-0025\n",
      "cleaning document:  LA081490-0030\n",
      "cleaning document:  LA081589-0043\n",
      "cleaning document:  LA081890-0039\n",
      "cleaning document:  LA091889-0088\n",
      "cleaning document:  LA092189-0123\n",
      "cleaning document:  LA092490-0095\n",
      "cleaning document:  LA092790-0010\n",
      "cleaning document:  LA093089-0076\n",
      "cleaning document:  LA093089-0126\n",
      "cleaning document:  LA100789-0007\n",
      "cleaning document:  LA101090-0017\n",
      "cleaning document:  LA101289-0194\n",
      "cleaning document:  LA101690-0040\n",
      "cleaning document:  LA102189-0151\n",
      "cleaning document:  LA102190-0045\n",
      "cleaning document:  LA103089-0043\n",
      "cleaning document:  LA103089-0070\n",
      "cleaning document:  LA110490-0184\n",
      "cleaning document:  LA110589-0082\n",
      "cleaning document:  LA110590-0038\n",
      "cleaning document:  LA120290-0163\n",
      "cleaning document:  LA120389-0130\n",
      "cleaning document:  LA121189-0017\n",
      "cleaning document:  notes.txt\n",
      "cleaning document:  SJMN91-06012224\n",
      "cleaning document:  SJMN91-06071022\n",
      "cleaning document:  SJMN91-06084228\n",
      "cleaning document:  SJMN91-06105230\n",
      "cleaning document:  SJMN91-06129119\n",
      "cleaning document:  SJMN91-06136305\n",
      "cleaning document:  SJMN91-06142126\n",
      "cleaning document:  SJMN91-06143070\n",
      "cleaning document:  SJMN91-06161012\n",
      "cleaning document:  SJMN91-06169114\n",
      "cleaning document:  SJMN91-06182091\n",
      "cleaning document:  SJMN91-06184003\n",
      "cleaning document:  SJMN91-06184021\n",
      "cleaning document:  SJMN91-06184088\n",
      "cleaning document:  SJMN91-06187248\n",
      "cleaning document:  SJMN91-06189077\n",
      "cleaning document:  SJMN91-06191081\n",
      "cleaning document:  SJMN91-06191174\n",
      "cleaning document:  SJMN91-06192123\n",
      "cleaning document:  SJMN91-06193081\n",
      "cleaning document:  SJMN91-06193235\n",
      "cleaning document:  SJMN91-06195131\n",
      "cleaning document:  SJMN91-06212161\n",
      "cleaning document:  SJMN91-06246065\n",
      "cleaning document:  SJMN91-06255434\n",
      "cleaning document:  SJMN91-06276078\n",
      "cleaning document:  SJMN91-06283083\n",
      "cleaning document:  SJMN91-06290146\n",
      "cleaning document:  SJMN91-06301029\n",
      "cleaning document:  SJMN91-06312120\n",
      "cleaning document:  WSJ870123-0101\n",
      "cleaning document:  WSJ870227-0149\n",
      "cleaning document:  WSJ870306-0171\n",
      "cleaning document:  WSJ870501-0141\n",
      "cleaning document:  WSJ870818-0002\n",
      "cleaning document:  WSJ870908-0047\n",
      "cleaning document:  WSJ871215-0109\n",
      "cleaning document:  WSJ871216-0037\n",
      "cleaning document:  WSJ880617-0024\n",
      "cleaning document:  WSJ880621-0079\n",
      "cleaning document:  WSJ880923-0163\n",
      "cleaning document:  WSJ890828-0011\n",
      "cleaning document:  WSJ900418-0193\n",
      "cleaning document:  WSJ900615-0131\n",
      "cleaning document:  WSJ900705-0145\n",
      "cleaning document:  WSJ900720-0113\n",
      "cleaning document:  WSJ900914-0127\n",
      "cleaning document:  WSJ900918-0121\n",
      "cleaning document:  WSJ910107-0139\n",
      "cleaning document:  WSJ910208-0130\n",
      "cleaning document:  WSJ910304-0002\n",
      "cleaning document:  WSJ910304-0005\n",
      "cleaning document:  WSJ910326-0090\n",
      "cleaning document:  WSJ910405-0154\n",
      "cleaning document:  WSJ910529-0003\n",
      "cleaning document:  WSJ910607-0063\n",
      "cleaning document:  WSJ910628-0109\n",
      "cleaning document:  WSJ910702-0078\n",
      "cleaning document:  WSJ910709-0115\n",
      "cleaning document:  WSJ910710-0123\n",
      "cleaning document:  WSJ910710-0148\n",
      "cleaning document:  WSJ910718-0143\n",
      "cleaning document:  WSJ911030-0008\n",
      "cleaning document:  WSJ911031-0012\n",
      "cleaning document:  WSJ911106-0109\n",
      "cleaning document:  WSJ911121-0136\n",
      "cleaning document:  WSJ911212-0080\n",
      "cleaning document:  WSJ911224-0085\n",
      "cleaning document:  WSJ920103-0037\n",
      "cleaning document:  WSJ920114-0145\n",
      "cleaning document:  WSJ920211-0036\n",
      "Number of exceptions:  8\n"
     ]
    }
   ],
   "source": [
    "duc_data = read_DUC_document(path='./DUC2001/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP830325-0143</td>\n",
       "      <td></td>\n",
       "      <td>million gallon crude oil spill tanker ran agro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP880217-0175</td>\n",
       "      <td>Some 40 members of Congress have joined with t...</td>\n",
       "      <td>coalit member congress announc wednesday plan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP880318-0051</td>\n",
       "      <td>Multitudes of native peoples, tourists and sci...</td>\n",
       "      <td>thousand peol prayed, cheered, danced, beat dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP880330-0119</td>\n",
       "      <td>Population experts say that little would chang...</td>\n",
       "      <td>two side tri forc chang 1990 censu get way, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP880331-0140</td>\n",
       "      <td>The unofficial tornado season runs from April ...</td>\n",
       "      <td>rumbl spring thunderstorm announc begin unoffi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document                                            Summary  \\\n",
       "0  AP830325-0143                                                      \n",
       "1  AP880217-0175  Some 40 members of Congress have joined with t...   \n",
       "2  AP880318-0051  Multitudes of native peoples, tourists and sci...   \n",
       "3  AP880330-0119  Population experts say that little would chang...   \n",
       "4  AP880331-0140  The unofficial tornado season runs from April ...   \n",
       "\n",
       "                                                Text  \n",
       "0  million gallon crude oil spill tanker ran agro...  \n",
       "1  coalit member congress announc wednesday plan ...  \n",
       "2  thousand peol prayed, cheered, danced, beat dr...  \n",
       "3  two side tri forc chang 1990 censu get way, re...  \n",
       "4  rumbl spring thunderstorm announc begin unoffi...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:06<00:00, 49.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "def clean_text(sentence):\n",
    "    # remove non alphabetic sequences\n",
    "    pattern = re.compile(r'[^a-z]+')\n",
    "    sentence = sentence.lower()\n",
    "    sentence = str.lower(pattern.sub(' ', sentence).strip())\n",
    "    \n",
    "    # Tokenize\n",
    "    word_list = word_tokenize(sentence)\n",
    "    \n",
    "    # stop words\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    # puctuation\n",
    "    punct = set(string.punctuation)\n",
    "    \n",
    "    # remove stop words\n",
    "    word_list = [word for word in word_list if word not in stopwords_list]\n",
    "    # remove very small words, length < 3\n",
    "    # they don't contribute any useful information\n",
    "    word_list = [word for word in word_list if len(word) > 2]\n",
    "    # remove punctuation\n",
    "    word_list = [word for word in word_list if word not in punct]\n",
    "    \n",
    "    # stemming\n",
    "    ps  = PorterStemmer()\n",
    "    word_list = [ps.stem(word) for word in word_list]\n",
    "    \n",
    "    # lemmatize\n",
    "    lemma = WordNetLemmatizer()\n",
    "    word_list = [lemma.lemmatize(word) for word in word_list]\n",
    "    # list to sentence\n",
    "    sentence = ' '.join(word_list)\n",
    "    \n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:07<00:00, 43.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# we'll use tqdm to monitor progress of data cleaning process\n",
    "# create tqdm for pandas\n",
    "tqdm.pandas()\n",
    "# clean text data\n",
    "\n",
    "duc_data['Text'] = duc_data['Text'].progress_apply(lambda x: clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:01<00:00, 224.49it/s]\n"
     ]
    }
   ],
   "source": [
    "duc_data['Summary'] = duc_data['Summary'].progress_apply(lambda x: clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DMT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5aa4f371553b793bd8ff710b490fbd6f9254f23fdb1215b65f7457dc1ac2b38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
